# Chapter 2: Evidence, Expertise, and AI: The New Infrastructure

## Introduction

A clinician is rounding with residents on a general medicine ward. They encounter a 68-year-old man with newly diagnosed type 2 diabetes and a history of myocardial infarction 5 years ago. One resident asks: "Should we start him on a statin?" Another replies: "Well, I think most diabetics get statins anyway."

The attending pauses. "But what's the evidence that a statin will reduce his future cardiovascular risk compared to diet and exercise alone, and how does that apply to _this_ patient specifically?" Neither resident has a clear answer. They turn to their phones, pull up an AI medical assistant, and within seconds receive a detailed recommendation with multiple citations.

But here's the problem: No one asked a clear question first. And no one will verify if the AI recommendation actually fits this specific patient.

This chapter is about understanding how the infrastructure for practicing evidence-based medicine has changed, what clinical expertise must become, and how to ask AI questions that produce useful answers. All three are connected: the infrastructure shaped what expertise looked like, and the new infrastructure requires rethinking expertise and how we interact with AI.

---

## Part 1: The Evolution of EBM Infrastructure

### The Traditional Evidence Bottleneck (Pre-1990s)

To understand what's changed, we need to understand what practicing medicine looked like before widespread computerized access to evidence.

In the mid-20th century, the medical literature exploded. By 1950, there were roughly 2,000 medical journals published worldwide. By 1975, over 3,000. By 1990, over 5,000. Meanwhile, clinicians were trained to make decisions based on:

- Textbooks (comprehensive but always out of date)
- Expert opinion (varied and often biased)
- Personal experience (memorable successes, forgotten failures)
- Pharmaceutical marketing (not objective)

The problem was obvious: Textbooks couldn't keep up. Experts disagreed. Personal experience was biased. Marketing wasn't objective.

**The infrastructure bottleneck was access.** If you wanted to search for evidence about a specific clinical question, you had limited options. Go to a medical library if you had access. Use PubMed on a desktop computer if you knew how. Call a medical librarian. Call a colleague. Or make a decision based on habit and memory.

For most clinicians, most clinical questions went unanswered in real time. Decisions were made by default: "This is what we've always done," "This is what my mentor taught me," or "This is what the pharmaceutical rep told me."

### The Internet-Era Solution (1995-2020)

The internet changed everything. Several key developments:

**PubMed went free (1996)**: The National Library of Medicine released PubMed as a free, publicly accessible search interface to MEDLINE. Suddenly, any clinician with internet access could search the entire indexed medical literature for free.

**Specialized databases emerged**: UpToDate, the Cochrane Library, Journal Club databases, professional guidelines, PubMed Central. Clinicians could not just search—they could find synthesized, pre-appraised evidence.

**The 6S hierarchy**: This natural hierarchy emerged for where to look for evidence:

1. Summaries (UpToDate, guidelines)
2. Synopses (Evidence-Based Medicine journal, ACP Journal Club)
3. Syntheses (Cochrane reviews, meta-analyses)
4. Studies (individual papers via PubMed)
5. Searches (customized PubMed searches)
6. Systems (integrated EHRs with decision support)

The beauty was matching effort to need: routine questions checked summaries (5 minutes), complex questions searched Cochrane (15 minutes), novel questions went to PubMed (30-60 minutes).

**What this required**: Access (increasingly universal), skills (literature search, understanding study design, critical appraisal), and time (still substantial).

**The bottleneck shifted** from "I can't find evidence" to "I found evidence, but synthesizing and appraising it requires substantial work."

### The AI-Era Disruption (2022 Onward)

In late 2022, ChatGPT became available. By 2024, multiple LLM-based medical AI systems existed. They did something new: synthesized evidence on demand.

You could ask a question in natural language. Within seconds: coherent synthesis, specific citations, recommendations, discussion of limitations.

**The infrastructure collapsed further.** The 6S hierarchy became less necessary. You didn't need to search databases or read synthesized summaries because the AI generated them instantly.

**New workflow**: Ask question → AI generates answer → Decide (if high-stakes: verify the answer)

### What Changed: The Core Shift

| |**Pre-Internet**|**Internet-Era**|**AI-Era**|
|---|---|---|---|
|**Access**|Hard, specialized|Easy, universal|Instant|
|**Speed**|Hours/days|Minutes|Seconds|
|**Synthesis**|Manual by clinician|Pre-synthesized|AI-synthesized|
|**Bottleneck**|Finding evidence|Synthesizing evidence|Verifying synthesis|


The critical insight: **We haven't eliminated the problem of evidence appraisal. We've relocated it.**

In the internet era, the problem was "How do I find and appraise evidence?" In the AI era, the problem is "How do I know if the AI's synthesis is correct?"

### What Was Traded Away

The move to AI-era infrastructure has changed what clinicians must engage with.

**Visibility of reasoning**: In the internet era, you could trace your decision back to specific papers you read. In the AI era, the AI's synthesis is opaque—you can't see exactly how it weighted different studies or why it emphasized certain evidence.

**Development of appraisal skills**: Learning critical appraisal required reading papers yourself. This built skills in recognizing bias, interpreting statistics, and judging evidence quality. With AI synthesis, these skills develop differently—now through verifying what the AI did rather than searching primary literature.

**Awareness of evidence gaps**: When you searched PubMed yourself, you learned what the evidence base covered and what it didn't. An AI giving you a comprehensive answer may hide the fact that critical questions lack evidence. You have to actively ask: "What's the AI not mentioning?"

**Reflection time**: The internet era was slower. This created space for thinking, discussing with colleagues, and letting decisions sit before acting. With instant answers, that reflection time is compressed.

For practicing EBM, this means you must be more intentional. You can't rely on accidental discoveries or the natural learning that came from working through evidence. You have to deliberately build these skills through verification and reflection.

---

## Part 2: Clinical Expertise in the AI Era

### What Clinical Expertise Always Meant

The original EBM framework defined clinical expertise as the ability to:

1. **Identify the patient's condition and individual risks** — Knowing what's actually going on with this patient and what could go wrong
2. **Recognize individual variation** — Understanding that this patient is not the average patient in the trial
3. **Understand patient values** — Knowing what matters to this patient. What are they willing to do? What are they unwilling to do?
4. **Integrate evidence with context** — Taking an evidence-based recommendation and adapting it to fit this specific person

This is still the definition. AI doesn't change it.

### What Changes in the AI Era

What does change is **where clinical expertise must intervene.**

In the pre-AI era:

- You got evidence (papers, textbooks, guidelines)
- You had to interpret it yourself
- You had to apply it to your patient
- Your clinical expertise was needed at multiple points

In the AI era:

- You get a synthesized recommendation from AI
- The evidence is already interpreted
- The recommendation is already formulated
- Your clinical expertise is needed at one specific point: **Deciding whether this AI recommendation fits this patient**

**The new role of clinical expertise**: AI gives you "Evidence supports X for your patient's condition." Clinical expertise asks: "But does X work for _this_ patient, with _their_ circumstances?"

### Examples of Clinical Expertise in Action

**Example 1: When Individual Baseline Risk Matters**

AI recommendation: "Start statin for primary prevention in diabetes"

Your patient: 48 years old, type 2 diabetes x 2 years, no smoking, normal blood pressure, normal lipids, no family history of early CAD, exercises regularly

Clinical expertise: This patient has low baseline cardiovascular risk. The statin trial showed benefit primarily in higher-risk patients. The number needed to treat for primary prevention in this patient is probably >500 over 10 years. Discussion with patient about whether this benefit is worth taking medication daily is warranted. This isn't a clear-cut "start statin" case.

**Example 2: When Trial Population Doesn't Match**

AI recommendation: "SGLT2 inhibitor for CKD stage 3"

Your patient: 82 years old, eGFR 45, multiple comorbidities, falls frequently, lives alone

Clinical expertise: The trials for SGLT2i in CKD studied relatively healthy patients without severe functional impairment. This frail patient with fall risk may have harms that outweigh benefits. The recommendation technically applies, but this patient is different from the trial population.

**Example 3: When Patient Values Change the Equation**

AI recommendation: "Start anticoagulation for atrial fibrillation. CHA₂DS₂-VASc score indicates benefit."

Your patient: Score of 4 (anticoagulation indicated). But patient explicitly says: "I don't want to take blood thinners. I've seen my mother bleed from them."

Clinical expertise: The patient has made clear their value is avoiding anticoagulation. You can't override that with evidence. You discuss the stroke risk and what that means for her, but you respect her choice. You might offer monitoring, aspirin, or other approaches that fit her preferences.

**Example 4: When You're Out of Your Depth**

AI recommendation: Complex management of a rare endocrine disorder

Your expertise: General internist. You've seen this condition maybe once in your career.

Clinical expertise: Recognizing you don't have enough experience to confidently apply this recommendation. You seek specialist input. You verify carefully. You don't rely on your clinical intuition because you don't have intuition about this rare condition.

### Recognizing Limits of Your Expertise

Clinical expertise includes knowing when you're in your area of competence and when you're beyond it.

**When you're in your area:** You've cared for many similar patients. You know the typical presentations, the typical responses to treatment, the typical pitfalls. You can apply an AI recommendation with confidence because you understand the landscape.

**When you're outside your area:** You're seeing a patient with a condition you rarely manage. You've seen maybe 2 similar patients ever. The AI gives a recommendation. Here, clinical expertise means: **Recognizing you don't have the pattern recognition to evaluate this well.** You should verify the recommendation more carefully. You might consult a specialist. You should be cautious about extrapolating from the general recommendation.

This is honest practice: Knowing what you know and don't know.

## Part 3: Verifying AI Output

When you receive an AI recommendation, you have three approaches to verify it before acting:

**1. Clinical Judgment: "Does this match what I expect?"**

This is pattern recognition from your experience. You've cared for similar patients. Does the AI's recommendation match what you'd expect clinically? Does it fit your patient?

When to use: You have substantial experience with this condition in your specialty. You can quickly assess if the recommendation makes sense.

Red flags: The recommendation doesn't match your experience. It ignores obvious patient factors. It contradicts what you've seen work.

**2. Source Appraisal: "Does the source actually support this?"**

Go find the sources the AI cites (studies, guidelines, meta-analyses). Read them. Do they say what the AI claims? Are they high-quality evidence? Do they apply to your patient?

When to use: High-stakes decisions. Recommendations outside your expertise. Any time you want to be sure.

This is the most reliable approach because you're checking the actual evidence, not relying on your judgment alone.

**3. Systematic Verification: "What's missing?"**

Ask: What didn't the AI mention? What alternatives exist? When would this not be recommended? What populations or scenarios does the evidence not cover?

When to use: Alongside clinical judgment or source appraisal. Helps catch gaps in reasoning.

### When to Use Which Approach

**Trust clinical judgment alone** when:

- It's in your area of expertise
- It's a routine decision (you've made similar 50+ times)
- The patient is typical
- It's low-stakes
- You're confident and can articulate why

**Do source appraisal** when:

- It's outside your expertise
- It's high-stakes (starting major medication, surgery, stopping treatment)
- The patient is atypical or rare presentation
- It contradicts your experience or current guidelines
- You're confident but can't articulate why

**Combine all three** when:

- Very high-stakes decisions
- Complex patients with multiple factors
- Recommendations you're unsure about
- Rare conditions outside your expertise

---

## Part 4: How to Ask AI Questions Effectively

To get useful answers from AI, you need to ask good questions. How you ask shapes what you get.

### How AI Systems Work (Clinician's Version)

**What LLMs are**: Large Language Models are trained on vast amounts of text: published literature, textbooks, websites, clinical notes. They predict statistically likely text based on patterns in training data.

**Key implications**:

- The AI can hallucinate (invent citations or details)
- The AI reflects biases in training data
- The AI sounds confident regardless of accuracy
- Knowledge cutoff matters (roughly April 2024 for most systems)
- Retrieval-Augmented Generation (RAG) systems can access current sources and databases, making them more reliable for recent information

### Principles for Good Prompts

**Be specific**: Include patient details, relevant comorbidities, specific clinical context

**Ask for evidence**: "What's the evidence base?" "Which studies or guidelines?"

**Ask for limitations**: "What are the limits of this evidence? What's not known?"

**Acknowledge complexity**: "I'm seeing conflicting information..." tells the AI this isn't a simple question

**Ask for alternatives**: "What are other options? What are the trade-offs?"

### Common Question Templates

1. **Guideline question**: "According to [guideline], what's recommended for [scenario]? What's the evidence base? What caveats apply?"
    
2. **Evidence quality**: "What's the strongest evidence for [intervention]? Study type? Sample size? Effect size? What's missing?"
    
3. **Alternatives**: "What are evidence-based options for [scenario]? Evidence for each? Trade-offs?"
    
4. **When not**: "When would you NOT recommend [intervention]? Contraindications? Weak evidence?"
    
5. **Conflicts**: "I'm seeing different recommendations for [topic]. What's the disagreement? Which has stronger evidence?"
    

### Following Up AI Answers

After the AI answers, verify with follow-up questions:

- "Where's this from?" (Get specific sources)
- "What would change your recommendation?" (Check for awareness of alternatives)
- "Does this apply when my patient has [specific factor]?" (Test for overgeneralization)
- "When would you NOT recommend this?" (Surface exceptions)
- "What's missing from this answer?" (Catch gaps)

### Red Flags in Prompts

Avoid:

- Asking about rare conditions without acknowledging rarity
- Asking about topics beyond knowledge cutoff without caveat
- Asking without patient context
- Asking the AI to judge its own confidence (doesn't work)

If different AI systems give different answers to the same question, that signals complexity. Disagreement means a simple answer doesn't exist.

---

## Summary

The infrastructure for practicing EBM has evolved through three eras:

**Pre-Internet (before 1995)**: Evidence was hard to access. EBM was aspirational for most clinicians. Infrastructure bottleneck: Access.

**Internet Era (1995-2022)**: Evidence was easily accessible. EBM became feasible for practicing clinicians. Infrastructure bottleneck: Synthesis and appraisal (required reading and thinking).

**AI Era (2022-present)**: Evidence is instantly synthesized by AI. Clinicians get answers in seconds. Infrastructure bottleneck: Verification (how do you know the AI is right?).

Clinical expertise remains essential throughout, but its role has shifted from "knowing things" to "knowing when evidence applies to this specific patient and how to verify AI's synthesis."

To practice safe EBM in this era, you need:

1. Understand AI infrastructure
2. Learn to ask questions effectively
3. Ability to Verify AI outputs
4. Knowledge of how to apply evidence to specific patients (Clinical Expertise)

The chapters that follow teach you how to appraise what the AI cites, apply evidence wisely to individual patients, and teach others to do the same.

---

## References


    

---

## Further Reading
