# Chapter 1: Asking Answerable Clinical Questions

## Introduction

A busy attending is rounding with two residents on a general medicine ward. They encounter a 68-year-old man with newly diagnosed type 2 diabetes mellitus and a history of myocardial infarction 5 years ago. One resident says, "Should we start him on a statin?" The other replies, "Well, I think most diabetics get statins anyway." The attending pauses and asks, "But what's the evidence that a statin will reduce his future cardiovascular risk compared to diet and exercise alone?" Neither resident has a clear answer. They turn to their phones, pull up an AI medical assistant, and within seconds receive a detailed recommendation with multiple citations. But here's the problem: no one has asked a clear question first.

This opening moment illustrates something fundamental to evidence-based medicine that hasn't changed in decades, despite the arrival of AI: the quality of your clinical decision depends entirely on the quality of the question you ask. A vague question yields vague answers. A well-formed question yields actionable ones. And in the age of AI, asking a good question is more critical than ever—because an AI system will confidently answer a bad question just as readily as a good one.

This chapter is about how to ask answerable clinical questions. The process hasn't fundamentally changed since the first edition of this book. But why we do it, and what to do with the answer once you have it, has shifted with the arrival of AI systems that can synthesize evidence instantly. We'll explore both the timeless aspects of question formulation and the new considerations that AI creates.

---

## Background and Foreground Questions

Clinical questions fall naturally into two categories based on what you're trying to understand: **background questions** and **foreground questions**.

### Background Questions: "What is this condition?"

Background questions ask about the nature of a disease or clinical phenomenon. They're the foundational "tell me about this" questions that medical education emphasizes.

**Examples of background questions:**

- What causes type 2 diabetes?
- How does the renin-angiotensin system work in heart failure?
- What is the pathophysiology of acute respiratory distress syndrome?
- What are the risk factors for deep vein thrombosis?

Background questions are **general knowledge questions**. They don't depend on your specific patient. They have answers that live in textbooks, review articles, and medical education materials. They're often best answered by traditional resources: textbooks, UpToDate, review articles in journals, or (yes) a well-targeted AI query.

The key insight is this: **background questions are not clinical decisions**. Knowing the pathophysiology of heart failure doesn't tell you whether _your_ patient with heart failure should get an ACE inhibitor. That's a different question entirely.

### Foreground Questions: "What should I do for this patient?"

Foreground questions are about clinical action. They're specific to your patient and ask what you should do next.

**Examples of foreground questions:**

- For this 68-year-old diabetic man with prior MI, should we start a statin?
- Should we give this patient with sepsis a vasopressor or fluid first?
- Is cognitive behavioral therapy more effective than antidepressants for this patient's depression?
- Should we perform imaging to rule out pulmonary embolism in this chest pain patient?

Foreground questions require **more than general knowledge**. They require evidence about **outcomes in populations like your patient**, evidence about the **effectiveness of specific interventions**, evidence about **what matters to your patient**.

### Why This Distinction Matters

In medical training, we spend enormous time on background questions. We learn pathophysiology in detail. We memorize mechanisms. This is necessary foundation-building. But when you're actually caring for a patient, you need foreground questions answered. And the type of evidence that answers them is different.

Here's where AI changes things: **AI systems are extremely good at synthesizing background information**. You can ask ChatGPT "What is type 2 diabetes?" and get a comprehensive, well-organized answer instantly. This is genuinely useful. But when you ask an AI system "Should _this_ patient get a statin?" without first asking a well-formulated foreground question, the AI will confidently provide an answer that sounds evidence-based but may not actually address your specific scenario.

---

## Background and Foreground Questions in Real Clinical Practice

Let's return to the ward round with the attending and the two residents. What actually happened was:

**Resident:** "Should we start him on a statin?"

This _sounds_ like a foreground question. But it's actually under-specified. The attending's response—asking about evidence—highlights the problem. Here's what the question is missing:

- **For whom?** A 68-year-old with type 2 diabetes and prior MI is not the same as a 45-year-old with newly diagnosed diabetes and no cardiovascular history.
- **Compared to what?** Starting a statin vs. not starting anything? vs. starting a different medication? vs. starting diet/exercise alone?
- **What outcome?** Preventing another MI? Reducing cardiovascular mortality? Reducing total mortality? Reducing any cardiovascular event?
- **In what timeframe?** Over the next year? Five years? Lifetime?
- **With what trade-offs?** What side effects is the patient willing to tolerate?

Until these questions are answered, the question "Should we start a statin?" is almost meaningless.

Now imagine the attending formulates this into a proper foreground question:

**"For a 68-year-old man with type 2 diabetes mellitus and a prior myocardial infarction 5 years ago, with LDL cholesterol of 120 mg/dL and no prior statin use, will starting a statin therapy reduce his risk of recurrent myocardial infarction or cardiovascular death over the next 5 years, and what are the side effects?"**

This is more specific. But it's also more useful—because now when the residents go look for evidence (or ask an AI system), they know what they're looking for. They can evaluate whether the evidence actually applies to this patient. They can form a real judgment about whether to proceed.

---

## Components of Answerable Clinical Questions

Foreground questions—the ones that drive clinical decisions—have a useful structure. Different variations exist, but they all try to specify four key components:

### 1. The Patient or Problem (P)

**What are the patient characteristics that define the population in question?**

Be specific. Don't just say "diabetic patient." Specify:

- Age (or age range)
- Type of diabetes (type 1 vs. type 2)
- Duration of diabetes
- Other relevant medical history (prior cardiovascular events, kidney disease, etc.)
- Baseline medication use
- Severity of disease (if relevant)

**Poor:** "Should we give this diabetic patient a statin?" **Better:** "For a 68-year-old with type 2 diabetes, prior MI, and LDL 120 mg/dL..."

Why does specificity matter? Because evidence from studies of young, newly diagnosed diabetics without cardiovascular disease may not apply to older diabetics with prior events. The more specifically you describe your patient, the better you can judge whether published evidence actually pertains to them.

### 2. The Intervention (I)

**What is the specific intervention, exposure, or test you're considering?**

Be concrete. Specify:

- The specific medication or therapy
- The dose (if relevant)
- The route of administration
- The duration or frequency
- Any relevant details about how it's given

**Poor:** "Should we give statins?" **Better:** "Starting atorvastatin 40 mg daily..."

Why specificity? Because the evidence for atorvastatin 40 mg daily differs from atorvastatin 80 mg daily or from rosuvastatin. Dose matters. Duration matters. These specifics determine whether the evidence applies to what you're actually doing.

### 3. The Comparator (C)

**What's the alternative you're comparing to?**

This is often missed, but it's critical. You're never deciding between "statin" and "nothing"—you're deciding between specific options.

**Options might be:**

- Statin vs. no statin
- Statin vs. diet/exercise alone
- One statin vs. a different statin
- Starting treatment now vs. waiting and monitoring

**Poor:** "Should we start a statin?" **Better:** "Atorvastatin 40 mg daily vs. no statin, with diet and exercise counseling in both groups"

Why does this matter? Because studies comparing statins to placebo look different from studies comparing high-dose to low-dose statins, which look different from studies comparing statins to diet/exercise alone. The evidence is specific to the comparison being made.

### 4. The Outcome (O)

**What do you actually care about?**

This is where many clinicians go astray. They look for evidence that a treatment "works," but they don't specify what "works" means.

**Outcomes might be:**

- Laboratory values (LDL cholesterol reduction)
- Clinical events (myocardial infarction)
- Surrogate endpoints (carotid intima-media thickness)
- Mortality (cardiovascular vs. total)
- Quality of life
- Side effects

These are very different. A statin _will_ reduce LDL cholesterol. That's a laboratory fact. But does reducing LDL reduce the risk of another MI in _this_ patient? Those are different questions requiring different evidence.

**Poor:** "Do statins work?" **Better:** "Reduce risk of recurrent myocardial infarction or cardiovascular death"

---

## Putting PICO Together: Well-Formed Foreground Questions

A well-formed foreground question includes all four components. Here's the statin example fully specified:

**Patient:** A 68-year-old man with type 2 diabetes mellitus, prior myocardial infarction 5 years ago, currently LDL cholesterol 120 mg/dL, no current statin use

**Intervention:** Atorvastatin 40 mg daily

**Comparator:** No statin therapy (with diet and exercise counseling)

**Outcome:** Reduction in risk of recurrent myocardial infarction, cardiovascular death, or any cardiovascular event over 5 years

**Full question:** "For a 68-year-old man with type 2 diabetes mellitus and a prior myocardial infarction 5 years ago (LDL 120 mg/dL, no prior statin use), will starting atorvastatin 40 mg daily (compared to no statin, with diet and exercise counseling in both groups) reduce his risk of recurrent myocardial infarction, cardiovascular death, or any cardiovascular event over the next 5 years?"

This is a real question. It's specific. It's answerable. And when you search for evidence—whether through traditional literature search or through an AI system—you'll know whether the evidence you find actually applies to your patient.

---

## Where and How Clinical Questions Arise

Clinical questions don't arise randomly. They emerge from the encounter with a patient, and they emerge from three types of situations:

### Questions Arising from Diagnosis

You see a patient with a symptom or finding you're trying to understand.

**Examples:**

- "Why does this patient have a persistent low-grade fever?"
- "What's causing this patient's elevated liver enzymes?"
- "Does this patient's chest pain indicate acute coronary syndrome?"

In these situations, your question is often about **diagnosis**: Is this condition present? What tests would help me figure that out? Is this test accurate enough to rely on?

**Foreground question format for diagnosis:**

- **Patient:** [Description of patient and presentation]
- **Intervention:** [Diagnostic test or clinical finding]
- **Comparator:** [Alternative test or no test]
- **Outcome:** [Sensitivity/specificity for identifying the condition; clinical usefulness]

Example: "In a 55-year-old woman with chest pain and normal EKG, does a high-sensitivity troponin test (compared to serial standard troponin) have better sensitivity for acute coronary syndrome and help reduce unnecessary hospital admissions?"

### Questions Arising from Treatment

You see a patient with a diagnosed condition and need to decide what to do.

**Examples:**

- "Should I start an ACE inhibitor for this patient's heart failure?"
- "Is it better to treat this patient's hypertension aggressively or moderately?"
- "Should this patient get chemotherapy or radiation for their cancer?"

In these situations, your question is about **therapy**: Does this treatment work? Is it better than alternatives? What are the side effects? Does it matter for outcomes my patient cares about?

**Foreground question format for therapy:**

- **Patient:** [Specific patient characteristics and disease severity]
- **Intervention:** [Specific treatment]
- **Comparator:** [Alternative treatment or standard care or placebo]
- **Outcome:** [Clinically meaningful outcomes: mortality, morbidity, quality of life]

Example: "For a 68-year-old with systolic heart failure (ejection fraction 35%), does starting an SGLT2 inhibitor (compared to standard ACE inhibitor + beta-blocker therapy) reduce mortality or hospitalizations over 2 years?"

### Questions Arising from Prognosis

You're trying to understand what will happen to this patient without intervention, or with a particular course of action.

**Examples:**

- "How likely is it that this patient's atrial fibrillation will lead to stroke?"
- "What's the 5-year survival for someone with this stage of cancer?"
- "Will this patient's chronic kidney disease progress to end-stage renal disease?"

In these situations, your question is about **prognosis**: What's the natural history? What factors predict outcomes? Should I change my management based on risk?

**Foreground question format for prognosis:**

- **Patient:** [Specific patient characteristics and disease stage]
- **Intervention/Exposure:** [Specific factor or marker you're assessing]
- **Comparator:** [Often implicit: patients with/without this factor]
- **Outcome:** [Long-term clinical outcome: mortality, morbidity, disease progression]

Example: "In a patient with newly diagnosed stage 3 chronic kidney disease and diabetes, what is the 5-year risk of progression to stage 4 CKD or end-stage renal disease, and which factors predict faster progression?"

---

## Our Reactions to Knowing and to Not Knowing

Before we move to how to actually find and use evidence, it's worth pausing on something psychological: How do we actually _feel_ about uncertainty, and how does that affect our questioning?

Clinical practice is fundamentally uncertain. You rarely have complete information. You make decisions with imperfect knowledge multiple times per day. Different clinicians have different relationships with this uncertainty.

### The Rush to Closure

Some clinicians experience uncertainty as deeply uncomfortable. They prefer to make a decision—any decision—rather than sit with not knowing. This leads to a cognitive shortcut: **premature closure**. You see a few features of the presentation, pattern-match to a diagnosis you know, and stop looking further.

**In clinical questioning, this manifests as:**

- Not asking the clarifying questions that would reveal you're missing something
- Accepting the first answer that fits, without checking if it's actually the best answer
- Not noticing when a patient's presentation doesn't quite fit the pattern you've chosen

Premature closure is dangerous. And it gets reinforced when you see a few patients with the same presentation who turned out to have your presumed diagnosis—you don't see the ones who had something different.

### The Paralysis of Perfect Information

Other clinicians are so aware of what they don't know that they become paralyzed. They want complete information before making any decision. They keep looking for more evidence, more tests, more information.

**In clinical questioning, this manifests as:**

- Asking so many detailed follow-up questions that you never settle on a management plan
- Ordering endless tests looking for the "one more thing" that will make the answer clear
- Delay in treatment while you seek perfect evidence

Paralysis is also dangerous—sometimes patients need treatment now, not after a year of investigation.

### Productive Uncertainty

The sweet spot is what we might call **productive uncertainty**: acknowledging what you don't know, asking good questions about it, but also accepting that perfect information is rarely available and making the best decision you can with available evidence and patient preferences.

This is where evidence-based medicine comes in. EBM gives you a framework for asking good questions about your uncertainty, finding the best available evidence about those questions, and integrating that evidence with your clinical judgment and patient values.

**Good clinical questioning requires:**

- Acknowledging uncertainty (you don't know the answer)
- Specificity (formulating exactly what you want to know)
- Actionability (asking something that will actually change what you do)
- Proportionality (matching the depth of your question to the stakes of the decision)

---

## Practicing Evidence-Based Medicine in Real Time

Here's where theory meets practice. You're in clinic or on rounds. You encounter a patient with a question you don't know the answer to. What do you actually do?

### The Real-Time Question

Most clinical questions arise in real time, during patient care. You don't have hours to research. You have minutes. Maybe seconds.

**Real example:** You're in clinic. A 45-year-old woman with hypothyroidism is asking whether she should try a gluten-free diet because she read online it might help her thyroid. You don't know the evidence on this. What do you do?

**Option 1 (Before AI era):** Say "I'm not sure, let me look into it and get back to you." Then later, search PubMed, find some papers, read abstracts, maybe full texts, synthesize what you learned, and email the patient back in a few days.

**Option 2 (AI era):** Pull out your phone, ask your AI assistant: "Is a gluten-free diet helpful for patients with hypothyroidism?" Get back a thoughtful answer in 10 seconds with citations. But then—and this is critical—you need to decide: Is this advice I can trust enough to give to this patient right now? Or do I need to verify it?

The second option seems obviously better in terms of speed. But it introduces a new problem: **How do you know if the AI answer is right?**

This is where the rest of this book comes in. But for now, just notice: **Even in the AI era, you still need to ask a good question first.**

If you ask the AI, "Is gluten-free good for thyroid?" you might get a rambling answer that covers thyroid disease, celiac disease, iodine content in gluten, and a bunch of other tangentially related information. If you ask: "In adults with autoimmune hypothyroidism without celiac disease, does a gluten-free diet improve thyroid function or symptoms?" you'll get a more focused answer.

### Real-Time Decision-Making Protocol

Here's a practical approach for real-time clinical questions:

**Step 1: Formulate the question**

- Take 30 seconds to think: What exactly do I want to know?
- Use PICO structure mentally (you don't need to write it out, but structure your thinking)
- Patient: Who is this? (Age, condition, relevant history)
- Intervention: What am I considering?
- Comparator: Compared to what?
- Outcome: What would matter to the patient?

**Step 2: Decide if you need to know right now**

- High-stakes decision (major medication change, surgery, stopping treatment)? → You likely need to verify the answer before acting
- Routine decision (dose adjustment, monitoring plan)? → You might be able to act on AI answer with follow-up verification
- Low-stakes question (patient education, background)? → You can probably use AI answer as-is

**Step 3: Find the answer**

- AI system (fast, but needs verification for high-stakes decisions)
- Your clinical knowledge (if you have relevant expertise)
- Asking a colleague (if available)
- Quick guideline check (if time permits)

**Step 4: If using AI or colleague, verify if needed**

- For high-stakes: Plan to verify the sources before acting (more on this in Chapter 5)
- For routine: Verify selectively (Chapter 5)
- For low-stakes: Verification less critical

**Step 5: Make the decision**

- Integrate the evidence with:
    - Your clinical judgment about this specific patient
    - The patient's values and preferences
    - Your clinical context (available resources, patient capacity, etc.)
- Document your reasoning
- Follow up as needed

### Teaching Questions for EBM in Real Time

If you're teaching residents or students, real-time questioning is where learning happens. Here's how to use clinical encounters as teaching moments:

**When a clinical question arises, pause and teach:**

1. **Name the question:** "This is a therapy question—does X treatment work for Y condition?"
    
2. **Help formulate it specifically:** "What exactly do we want to know? For _this_ patient? Compared to what? What outcomes matter?"
    
3. **Ask them to predict the answer:** "Before we look anything up, what do you think? Based on what you know, what would you guess?"
    
4. **Then look for evidence:** "Now let's see what the evidence says. How would you find it?"
    
5. **Compare to their prediction:** "So you predicted X, but the evidence says Y. Why do you think that is? What were you missing?"
    
6. **Apply to the patient:** "Does this evidence apply to our specific patient? Why or why not?"
    

This is Socratic teaching, and it's one of the most effective ways to build EBM thinking. It takes a few extra minutes on rounds, but it embeds the skill of question-asking into everyday practice.

---

## Why Bother Formulating Questions Clearly?

This might seem like pedantry. Why spend time formulating a perfect question when you could just ask an AI and get an answer in 5 seconds?

Here are the real reasons:

### 1. Clear Questions Get Better Answers

Whether you're searching a database, consulting an AI system, or asking a colleague, a specific question gets a better answer than a vague one. AI systems, in particular, are strongly influenced by how you ask. The same clinical scenario will yield different answers depending on how you phrase the question.

**Vague:** "Is SGLT2i good for diabetics?" **Specific:** "In a 60-year-old with type 2 diabetes, CKD stage 3, and prior heart failure, does adding SGLT2i reduce hospitalization compared to standard therapy?"

The second question will elicit a more focused, patient-relevant answer from any source.

### 2. Clear Questions Help You Judge Applicability

When you ask a clear question, you can better judge whether the evidence you find actually applies to your patient. If your question specifies "stage 3 CKD" and the evidence you find only includes patients with stage 4-5 CKD, you immediately recognize the mismatch.

If your question is vague, you might accept evidence that doesn't actually apply because you're not clear on what you were asking in the first place.

### 3. Clear Questions Protect Against Confirmation Bias

A well-specified question helps you avoid finding only evidence that supports what you already think. If you're looking for "does SGLT2i work?" you'll easily find papers showing it works. You might miss papers showing it doesn't work in certain populations, or that side effects are common.

A specific question—with specified population, intervention, comparator, and outcome—makes you look for evidence that directly addresses what you want to know, not just evidence that validates a decision you've already made.

### 4. Clear Questions Make Your Reasoning Transparent

When you document the question you asked, your clinical reasoning becomes clear to others. If someone reviews your chart and sees "Question: In 68yo M with DM2, prior MI, and LDL 120, does atorvastatin 40mg reduce recurrent MI vs. no statin?"—they understand exactly what you were thinking and can judge whether that reasoning applies to this patient.

Vague reasoning is hard to teach from and hard to defend.

### 5. Clear Questions Acknowledge Your Uncertainty

By asking a specific question, you're acknowledging: "Here's what I know. Here's what I don't know. Here's what I want to find out." This is more intellectually honest than pretending to have certainty you don't have.

Patients respect clinicians who acknowledge uncertainty and try to resolve it. Colleagues respect clinicians who ask good questions rather than offering unexamined opinions.

---

## How Question Precision Filters AI Output and Hallucination

This is new territory, so let's address it head-on: One of the major challenges with AI in medicine is that AI systems can sound very confident while being completely wrong. They can cite studies that don't exist, misrepresent findings, or generalize far beyond what evidence supports.

A vague question makes this problem worse. If you ask "Is SGLT2i good for diabetics?" an AI might return:

- Information about SGLT2i in type 1 and type 2 diabetes
- Data on cardiovascular benefits, renal benefits, and weight loss
- Information about side effects
- All of it potentially accurate, but a confusing synthesis that doesn't answer what _you_ actually needed to know

An AI might even hallucinate—confidently cite a study that doesn't exist, or misrepresent what a real study found.

But here's the thing: **A precise question acts as a filter on AI output.** When you ask specifically about "stage 3 CKD," the AI is more likely to retrieve evidence about stage 3 specifically (rather than conflating stages). When you specify the outcome you care about, the AI focuses on that outcome.

Even better: **A precise question helps you detect when the AI is wrong.** If you ask "Does SGLT2i reduce progression to ESRD in stage 3 CKD?" and the AI cites a study about type 2 diabetes generally, you immediately notice the mismatch. If the question is vague, you might not notice.

**Later chapters will teach you how to verify AI output thoroughly (Chapter 5: Source Appraisal). But the first defense against AI errors is asking a precise question.** A vague question is a form of intellectual laziness that exposes you to AI hallucination. A precise question forces both you and the AI to be specific.

---

## Common Pitfalls in Question Formulation

Let's look at some mistakes people commonly make:

### Pitfall 1: The Unanswerable Question

**"What should I do?"** This is too broad. It invites answers that aren't evidence-based.

**Better:** "Will starting therapy X improve outcome Y compared to standard care?"

### Pitfall 2: The Question You Don't Actually Want Answered

**"Is this disease serious?"** You probably don't want a general answer about the disease. You want to know about prognosis in _your_ patient with _their_ specific features.

**Better:** "What's the 5-year mortality for a 75-year-old with stage 3 CKD and diabetes, and what factors predict worse outcomes?"

### Pitfall 3: The Laboratory Value Question When You Mean Clinical Outcome

**"Does this drug improve the biomarker?"** That's not what you care about. You care about clinical outcomes.

**Better:** "Does lowering this biomarker correlate with improved clinical outcomes?" Or: "Does therapy to reduce this biomarker prevent clinical complications?"

### Pitfall 4: The Comparison Question That Isn't Compared

**"Does therapy X work?"** Compared to what? Placebo? No treatment? Standard treatment? Different doses of the same drug?

**Better:** "Does therapy X (compared to Y) improve outcome Z?"

### Pitfall 5: The Outcome Question That Isn't Specific

**"Is this treatment effective?"** Effective at what? Reducing symptoms? Preventing disease? Extending life? Improving quality of life? These are all different.

**Better:** "Does this treatment reduce risk of [specific clinical event] by [clinically meaningful amount]?"

---

## Practicing Question Formulation: Exercises

If you're using this with a teaching group, here are some exercises to build question formulation skills:

### Exercise 1: Convert Vague to Specific

Take a vague clinical question and convert it to PICO format:

**Vague:** "Are statins good for diabetics?"

**Specific:** "In adults with type 2 diabetes without prior cardiovascular disease (primary prevention), does statin therapy (compared to placebo or no treatment) reduce risk of myocardial infarction, stroke, or cardiovascular death?"

Try these:

- "Should we screen for osteoporosis?"
- "Is metformin safe?"
- "Do antidepressants work for anxiety?"

### Exercise 2: Diagnose the Missing Component

Each of these questions is missing a component of PICO. Identify what's missing:

1. "In a 70-year-old man with hypertension, will starting an ACE inhibitor reduce blood pressure?" _(Missing: specific comparator—ACE inhibitor compared to what?)_
    
2. "Does cognitive behavioral therapy improve depression?" _(Missing: patient specificity—in whom? Mild/moderate/severe? First episode or recurrent?)_
    
3. "For a patient with newly diagnosed type 2 diabetes, will metformin prevent complications?" _(Missing: specific outcome—prevent which complications? Over what timeframe?)_
    

Try formulating what the complete questions should be.

### Exercise 3: Real Patient Scenario

A patient comes to you with a concern. Formulate the clinical question:

**Scenario:** A 52-year-old woman with newly diagnosed atrial fibrillation (paroxysmal, no structural heart disease, CHADS2 score 1) is asking whether she needs anticoagulation to prevent stroke. She's concerned about bleeding risk.

**Your question:**

- Patient: ?
- Intervention: ?
- Comparator: ?
- Outcome: ?

Try this approach with several real scenarios from your practice.

---

## Summary

Asking a clear, answerable clinical question is the foundation of evidence-based medicine. It hasn't changed with the advent of AI, but the stakes are higher: **a precise question helps you filter AI output and detect when the AI is wrong.**

The key components of a foreground (action-oriented) clinical question are:

1. **Patient (P)**: Specific description of the person and their condition
2. **Intervention (I)**: The specific action, treatment, or test you're considering
3. **Comparator (C)**: What you're comparing it to
4. **Outcome (O)**: What you actually care about measuring

These components can be organized as a simple question that guides your thinking and helps you find and evaluate evidence.

In the next chapter, we'll look at how the infrastructure for finding and evaluating evidence has changed with the rise of AI—and why understanding that change is critical for practicing EBM in 2024 and beyond.

---

## References


---

## Further Reading

